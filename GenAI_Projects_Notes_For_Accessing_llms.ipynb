{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNeE1/JDPkDr3UDESEslis",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devanshmishra10/GenAI_Projects/blob/main/GenAI_Projects_Notes_For_Accessing_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKw8izpWhTLl",
        "outputId": "ba532bc8-35c4-4ceb-b392-122833c8bd62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/948.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/948.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m942.1/948.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "T6Eo3sAPhsYy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "response= client.responses.create(\n",
        "  model=\"gpt-4.1-2025-04-14\",\n",
        "  input=\"Tell me a three sentence story about a cat\"\n",
        ")\n",
        "print(response.output[0].content[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOOm05UbiYgX",
        "outputId": "6787d167-d20a-43db-9825-b2f56e143f3c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whiskers the cat found a magical, glowing pebble in the garden. When she touched it with her paw, she suddenly understood the language of birds. From that day on, Whiskers became the wisest, most well-informed cat in her neighborhood.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat Completion"
      ],
      "metadata": {
        "id": "hh1-L_QclPXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "chat_history=[\n",
        "      {\"role\":\"system\",\"content\":\"You are a helpful assistant\"},\n",
        "      {\"role\":\"user\",\"content\":\"What is ML?\"}\n",
        "  ]\n",
        "response= client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=chat_history\n",
        ")\n",
        "output=response.choices[0].message.content\n",
        "print(output)\n",
        "chat_history.append({\"role\":\"assistant\",\"content\":output})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnZhxa9OiYdW",
        "outputId": "f8d84224-9692-407a-bc93-adb1e98be327"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ML stands for Machine Learning. It is a branch of artificial intelligence that involves creating algorithms and models that allow computers to learn from data and make predictions or decisions without being explicitly programmed. Machine Learning is used in various fields such as healthcare, finance, marketing, and more to analyze complex data and automate tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GEMINI**"
      ],
      "metadata": {
        "id": "Jjt6yeQenPaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YznD3NeAiYOP",
        "outputId": "1ac043b7-9a7a-484c-9dcf-7d35ef953b8e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.7/244.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "GKm5zO07nXIV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "client = genai.Client(api_key=gemini_api_key)\n",
        "\n",
        "response=client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain how AI works?\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "njx6PfoQnW7a",
        "outputId": "5845c06e-0788-44bf-d233-22a6c8ee4ee2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At its core, **Artificial Intelligence (AI)** works by enabling machines to perform tasks that typically require human intelligence. This isn't about giving computers consciousness or feelings, but rather equipping them with the ability to **learn, reason, perceive, and act** in ways that mimic human cognitive functions.\n",
            "\n",
            "Here's a breakdown of the fundamental principles:\n",
            "\n",
            "## The Core Ingredients of AI\n",
            "\n",
            "Think of AI as a sophisticated recipe with a few key ingredients:\n",
            "\n",
            "1.  **Data (The Fuel):**\n",
            "    *   AI models learn from vast amounts of information – text, images, audio, sensor readings, numbers, etc. This data is the \"experience\" that allows the AI to understand patterns and relationships.\n",
            "    *   **Example:** To recognize a cat, an AI needs to see millions of images of cats (and non-cats) to learn what features define a cat.\n",
            "\n",
            "2.  **Algorithms & Models (The Recipe/Brain):**\n",
            "    *   These are the mathematical rules and computational procedures that the AI uses to process data, identify patterns, make decisions, and learn.\n",
            "    *   The \"model\" is the specific instantiation of an algorithm that has been trained on data. It's the trained \"brain\" of the AI system.\n",
            "    *   **Examples:** Neural networks (especially deep neural networks), decision trees, support vector machines, regression models.\n",
            "\n",
            "3.  **Computational Power (The Engine):**\n",
            "    *   Processing massive datasets and executing complex algorithms requires significant computing power, often provided by specialized hardware like GPUs (Graphics Processing Units) and cloud computing platforms.\n",
            "\n",
            "## How AI Learns and Works (The Process)\n",
            "\n",
            "The process typically involves these steps:\n",
            "\n",
            "1.  **Training (The Learning Phase):**\n",
            "    *   **Input Data:** The AI model is fed a large dataset.\n",
            "    *   **Pattern Recognition:** The algorithm analyzes this data to find hidden patterns, correlations, and relationships.\n",
            "    *   **Learning from Error (Supervised Learning):** If it's a \"supervised\" learning task (most common), the data includes both the input and the correct output (e.g., an image of a cat *labeled* \"cat\").\n",
            "        *   The model makes a prediction.\n",
            "        *   It compares its prediction to the correct answer.\n",
            "        *   It calculates the \"error\" or difference.\n",
            "        *   It then adjusts its internal parameters (weights and biases in a neural network) to reduce this error for future predictions. This adjustment is done iteratively, often thousands or millions of times.\n",
            "    *   **Learning without Labels (Unsupervised Learning):** In some cases, the data isn't labeled. The AI tries to find natural groupings or structures within the data itself (e.g., clustering customers into different segments based on their purchasing behavior).\n",
            "    *   **Learning by Trial and Error (Reinforcement Learning):** Here, an AI agent learns by interacting with an environment. It receives rewards for desired actions and penalties for undesirable ones, slowly learning the optimal strategy (e.g., AI playing a video game).\n",
            "\n",
            "2.  **Inference (The Application Phase):**\n",
            "    *   Once the model is trained, it's ready to be used.\n",
            "    *   **New Input:** When you provide the trained AI with new, unseen data (e.g., a new image), it uses the patterns it learned during training to make a prediction or take an action.\n",
            "    *   **Output:** The model processes the input through its learned parameters and generates an output (e.g., identifying the object in the image, translating text, recommending a product).\n",
            "\n",
            "3.  **Feedback & Refinement (Continuous Improvement):**\n",
            "    *   AI systems can often be continuously refined. New data, user feedback, or performance monitoring can be used to retrain or fine-tune the model, making it more accurate and robust over time.\n",
            "\n",
            "## A Simple Analogy: Learning to Identify Apples\n",
            "\n",
            "Imagine you want to teach a child (AI) to identify an apple:\n",
            "\n",
            "1.  **Data:** You show the child hundreds of pictures of apples (red, green, big, small, whole, sliced) and non-apples (bananas, oranges, balls). You tell them \"This is an apple\" or \"This is NOT an apple.\" (Labeled data).\n",
            "2.  **Algorithms/Model:** The child's brain (the AI model) starts to form internal rules: \"Apples are usually round-ish, often red or green, have a stem, etc.\"\n",
            "3.  **Training:** The child keeps practicing. If they say \"apple\" for a banana, you correct them. They adjust their internal rules based on the feedback. They learn to ignore irrelevant features (like background color) and focus on defining features (shape, stem).\n",
            "4.  **Inference:** Later, you show the child a new fruit they've never seen before. They apply their learned rules and confidently say, \"That's an apple!\"\n",
            "5.  **Refinement:** If they later see a very unusual apple and make a mistake, you correct them, and they refine their rules even further.\n",
            "\n",
            "## Key Types of AI (Based on How They Learn)\n",
            "\n",
            "*   **Machine Learning (ML):** The overarching category, where systems learn from data without explicit programming.\n",
            "    *   **Deep Learning (DL):** A subset of ML that uses multi-layered neural networks (inspired by the human brain) to learn complex patterns, especially effective with unstructured data like images, audio, and text. This is what powers most breakthroughs in AI today (e.g., facial recognition, self-driving cars, large language models like ChatGPT).\n",
            "*   **Natural Language Processing (NLP):** Enables computers to understand, interpret, and generate human language (e.g., translation, sentiment analysis, chatbots).\n",
            "*   **Computer Vision (CV):** Allows computers to \"see\" and interpret visual information from images and videos (e.g., object detection, medical image analysis).\n",
            "*   **Robotics:** Integrates AI with physical machines to perform tasks in the real world.\n",
            "\n",
            "In essence, AI works by using sophisticated mathematical algorithms and massive amounts of data to enable computers to learn patterns, make decisions, and solve problems in ways that simulate human intelligence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multi turn conversations\n",
        "\n",
        "chat = client.chats.create(model=\"gemini-2.5-flash\")\n",
        "response = chat.send_message(\"I have 2 dogs in my house\")\n",
        "print(response.text)\n",
        "\n",
        "print(\"-\"*50)\n",
        "response=chat.send_message(\"How many dogs do I have?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rcnnrRAo3cR",
        "outputId": "1f9c69ba-8b17-4d0b-e53c-ed2a01247d3f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's wonderful! Two dogs usually means double the cuddles and double the fun (and sometimes double the mischief!).\n",
            "\n",
            "What are their names? Or what kind of dogs are they?\n",
            "--------------------------------------------------\n",
            "Based on your previous statement, you have **2** dogs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GROQ**"
      ],
      "metadata": {
        "id": "z2Dx7y1apyOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhX2d9Nyp2xA",
        "outputId": "89614dad-016a-4123-e536-bb46ebd7b2ac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "D3VRZXs6qiQW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=groq_api_key)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant\"\n",
        "        }, # Add a comma here\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of DL\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "qxBtGwDKp2gx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LANGCHAIN**"
      ],
      "metadata": {
        "id": "clpTH0yltvH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-community"
      ],
      "metadata": {
        "id": "FnR0jan1t1ej"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeHh7MXKvBDc",
        "outputId": "2bfa3574-a18e-4124-b224-ce7cca85e5c1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    api_key=api_key,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"What is ML?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJBRzOSlvYTe",
        "outputId": "886567cb-927b-47ba-acfd-52aa25431823"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method BaseMessage.text of AIMessage(content='ML stands for machine learning, which is a subset of artificial intelligence that focuses on developing algorithms and models that allow computers to learn from and make predictions or decisions based on data without being explicitly programmed to do so. Machine learning algorithms can analyze and interpret large amounts of data to identify patterns, make predictions, and automate decision-making processes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 11, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CLYoGxI9j5p3v9KIpXXmF3j3kbHcE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--534cc3ff-7079-41a1-af69-6cddfffcf177-0', usage_metadata={'input_tokens': 11, 'output_tokens': 66, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRzcBH8XwUzn",
        "outputId": "a4c0a16e-1ce5-4515-831b-ec6b7933ef82"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=gemini_api_key,\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"What is ML?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEtmRmlPwUXH",
        "outputId": "f7bcdbb3-a114-45ab-ee33-02afbfc88461"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method BaseMessage.text of AIMessage(content='**Machine Learning (ML)** is a subfield of Artificial Intelligence (AI) that enables systems to learn from data, identify patterns, and make decisions or predictions with minimal human intervention, **without being explicitly programmed for every task.**\\n\\nThink of it this way:\\n\\n*   **Traditional Programming:** You write specific, step-by-step instructions for the computer to follow. If you want it to identify a cat, you\\'d have to write code for \"if it has pointy ears AND whiskers AND fur AND meows, then it\\'s a cat.\" This is very difficult and often impossible for complex tasks.\\n*   **Machine Learning:** Instead of giving explicit instructions, you give the computer a lot of examples (data) and an algorithm. The algorithm then \"learns\" the patterns and rules from that data itself. So, to identify a cat, you\\'d show it thousands of pictures of cats (and non-cats), and the ML model would figure out what features distinguish a cat.\\n\\n---\\n\\n### How Does ML Work (Simplified)?\\n\\n1.  **Data:** ML models need data to learn. This data can be anything: images, text, numbers, audio, etc. The more relevant and high-quality data, the better the learning.\\n2.  **Algorithm:** This is the \"learning recipe\" or mathematical procedure that the computer uses to find patterns in the data. There are many different types of algorithms (e.g., neural networks, decision trees, support vector machines).\\n3.  **Training:** The algorithm processes the data, looking for relationships, correlations, and structures. During this phase, the model adjusts its internal parameters to minimize errors in its predictions.\\n4.  **Model:** The output of the training process is a \"model.\" This model is essentially the learned knowledge – it has captured the patterns from the data.\\n5.  **Prediction/Decision:** Once trained, the model can be given new, unseen data. It uses its learned knowledge to make a prediction or a decision based on that new input.\\n6.  **Improvement:** The model\\'s performance can be evaluated, and with more data or adjustments to the algorithm, it can continue to improve over time.\\n\\n---\\n\\n### Key Types of Machine Learning:\\n\\n1.  **Supervised Learning:**\\n    *   **Concept:** Learning from labeled data, like a student learning with a teacher. Each piece of input data has a corresponding \"correct answer\" or label.\\n    *   **Examples:**\\n        *   **Classification:** Is this email spam or not spam? (Yes/No)\\n        *   **Regression:** What will the price of this house be? (A continuous number)\\n    *   **Applications:** Image recognition, medical diagnosis, fraud detection.\\n\\n2.  **Unsupervised Learning:**\\n    *   **Concept:** Learning from unlabeled data, like a student exploring on their own to find hidden structures or patterns without explicit guidance.\\n    *   **Examples:**\\n        *   **Clustering:** Grouping customers into different segments based on their purchasing behavior.\\n        *   **Dimensionality Reduction:** Simplifying complex data while retaining important information.\\n    *   **Applications:** Customer segmentation, anomaly detection, recommendation systems.\\n\\n3.  **Reinforcement Learning:**\\n    *   **Concept:** Learning by trial and error, like a child learning to ride a bike. An \"agent\" takes actions in an environment and receives rewards or penalties, learning to maximize rewards over time.\\n    *   **Applications:** Game playing (e.g., AlphaGo), robotics, autonomous vehicles.\\n\\n4.  **Deep Learning:**\\n    *   **Concept:** A *subset* of machine learning that uses artificial neural networks with many layers (hence \"deep\") to learn complex patterns from vast amounts of data. It\\'s particularly effective for tasks involving images, audio, and text.\\n    *   **Applications:** Face recognition, natural language processing (like ChatGPT), self-driving cars.\\n\\n---\\n\\n### Why is ML Important?\\n\\n*   **Automation:** Automates tasks that are too complex or time-consuming for humans.\\n*   **Insights:** Uncovers hidden patterns and insights in large datasets that would be impossible for humans to find.\\n*   **Personalization:** Powers recommendation systems, personalized content, and targeted advertising.\\n*   **Prediction:** Forecasts future trends, stock prices, weather, and more.\\n*   **Adaptability:** Creates systems that can adapt and improve over time without constant reprogramming.\\n\\nIn essence, ML is about empowering computers to learn from experience, just like humans do, opening up a world of possibilities for solving complex problems and creating intelligent systems.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b9520d16-1d95-4df3-b61b-a67e48ff2e25-0', usage_metadata={'input_tokens': 5, 'output_tokens': 2052, 'total_tokens': 2057, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1076}})>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-groq"
      ],
      "metadata": {
        "id": "iYg05s9zwvlO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLamaIndex**"
      ],
      "metadata": {
        "id": "p_mpwM1Wxh6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn0Kuxx2wvUe",
        "outputId": "5833166c-3426-4fd5-854c-a8eef4be91fd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U llama-index-llms-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WULyWRD-wvFp",
        "outputId": "7e5da521-53d9-4e57-a899-0cce1805bea2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-cli 0.5.1 requires llama-index-llms-openai<0.6,>=0.5.0, but you have llama-index-llms-openai 0.6.1 which is incompatible.\n",
            "llama-index 0.14.3 requires llama-index-llms-openai<0.6,>=0.5.0, but you have llama-index-llms-openai 0.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U llama-index-llms-google-genai"
      ],
      "metadata": {
        "id": "xYvgZ0kKx1km"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "BoXKjZKYx1d-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "llm = OpenAI(api_key=api_key, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "response = llm.complete(\"ML is\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUwCJnWnyUAu",
        "outputId": "ca149972-b780-4e39-e4d7-eb1c490dfe42"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " a branch of artificial intelligence that involves the development of algorithms and models that can learn from and make predictions or decisions based on data. It is used in a wide range of applications, such as image and speech recognition, natural language processing, and recommendation systems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "llms= OpenAI(api_key=api_key, model=\"gpt-3.5-turbo\")\n",
        "messages=[\n",
        "    ChatMessage(role=\"system\", content=\"You are a helpful assistant\"),\n",
        "    ChatMessage(role=\"user\", content=\"What is ML?\")\n",
        "]\n",
        "\n",
        "response=llms.chat(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoTL5LavyT9R",
        "outputId": "b9208cba-7334-4249-e4bf-8634efcb3f2f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: ML stands for machine learning. It is a subset of artificial intelligence that focuses on the development of algorithms and models that allow computers to learn and make predictions or decisions based on data without being explicitly programmed. Machine learning algorithms can analyze and learn from data to identify patterns, make predictions, and improve performance over time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Differences Summarized:\n",
        "Feature\n",
        "LlamaIndex\n",
        "LangChain\n",
        "Primary Focus\n",
        "Data indexing and retrieval for RAG\n",
        "Orchestration and building diverse LLM applications\n",
        "Data Handling\n",
        "Specialized for ingesting, structuring, and efficiently querying custom data\n",
        "Handles data loading and processing but with a broader focus on application flow\n",
        "Complexity\n",
        "More streamlined for RAG systems and connecting to data\n",
        "Designed for building complex, multi-step workflows and agentic applications\n",
        "Flexibility\n",
        "Optimized for data-centric LLM applications\n",
        "Offers extensive flexibility for various LLM use cases and integrations"
      ],
      "metadata": {
        "id": "mInCqER0z5lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ques=input(\"Enter question: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFOx8nbhyqQV",
        "outputId": "fff4cf14-1986-4d44-f7a8-5454d962ab97"
      },
      "execution_count": 51,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter question: How is the day?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WvXxJFI90v5L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}